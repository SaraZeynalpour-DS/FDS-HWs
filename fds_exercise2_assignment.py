# -*- coding: utf-8 -*-
"""FDS_Exercise2_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eFgmXHjz6pi3dI0lwgmof8-cp7ZSZp79

# Fundamentals of Data Science
Winter Semester 2021

## Prof. Fabio Galasso, Guido D'Amely, Alessandro Flaborea, Luca Franco, Muhammad Rameez Ur Rahman and Alessio Sampieri
<galasso@di.uniroma1.it>, <damely@di.uniroma1.it>, <flaborea@di.uniroma1.it>, <franco@diag.uniroma1.it>, <rahman@di.uniroma1.it>, <alessiosampieri27@gmail.com>


## Exercise 2: Classification

In Exercise 2, you will re-derive and implement logistic regression and optimize the parameters with Gradient Descent and with the Newton's method. Also, in this exercise you will re-derive and implement Gassian Discriminant Analysis.
We will use datasets generated from the make_classification function from the SkLearn library. Its first output contains the feature values $x^{(i)}_1$ and $x^{(i)}_2$ for the $i$-th data sample $x^{(i)}$. The second contains the ground truth label $y^{(i)}$ for each corresponding data sample.

The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.
Submit it by sending an email to galasso@di.uniroma1.it, flaborea@di.uniroma1.it, franco@diag.uniroma1.it and alessiosampieri27@gmail.com  by Wednesday November 17th 2021, 23:59.

## Notation

- $x^i$ is the $i^{th}$ feature vector
- $y^i$ is the expected outcome for the $i^{th}$ training example
- $m$ is the number of training examples
- $n$ is the number of features

Let's start by setting up our Python environment and importing the required libraries:
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np # imports a fast numerical programming library
import scipy as sp # imports stats functions, amongst other things
import matplotlib as mpl # this actually imports matplotlib
import matplotlib.cm as cm # allows us easy access to colormaps
import matplotlib.pyplot as plt # sets up plotting under plt
import pandas as pd # lets us handle data as dataframes
from sklearn.datasets import make_classification
import seaborn as sns

# sets up pandas table display
pd.set_option('display.width', 500)
pd.set_option('display.max_columns', 100)
pd.set_option('display.notebook_repr_html', True)

import seaborn as sns # sets up styles and gives us more plotting options

"""## Question 1: Logistic Regression with Gradient Ascent **(10 Points)**

### Code and Theory

#### Exercise 1.a **(3 Points)** Equations for the log likelihood, its gradient, and the gradient ascent update rule.

Write and simplify the likelihood $L(\theta)$ and log-likelihood $l(\theta)$ of the parameters $\theta$.

Recall the probabilistic interpretation of the hypothesis $h_\theta(x)= P(y=1|x;\theta)$ and that $h_\theta(x)=\frac{1}{1+\exp(-\theta^T x)}$.

Also derive the gradient $\frac{\delta l(\theta)}{\delta \theta_j}$ of $l(\theta)$ and write the gradient update equation. 

Question: Are we looking for a local minimum or a local maximum using the gradient ascent rule?

################# Do not write above this line #################

#####**Question 1.a answers:**
*For the first part of the quastion:*

*Likelihood:*

$P(y=1|x;\theta)= h_\theta(x)$

$P(y=0|x;\theta) = 1 - h_\theta(x)$

$P(y|x;\theta) = (h_\theta(x))^y (1-h_\theta(x))^{1-y}$


* Since each datapoint is independent, the probability of all the data is:



$L(\theta) = P(\vec{y}|x;\theta) = {\displaystyle \prod_{i=1}^{m} P(y^{(i)}|x^{(i)};\theta)} = {\displaystyle \prod_{i=1}^{m} h_\theta(x^{(i)})^{y(i)}{(1-h_\theta(x^{(i)}))}^{1-y(i)}}$

$L(\theta) = {\displaystyle \prod_{i=1}^{m}{(\frac{1}{1+\exp(-\theta^T x^{(i)})})}^{y(i)}{(1-\frac{1}{1+\exp(-\theta^T x^{(i)})})}^{1-y(i)}} = {\displaystyle \prod_{i=1}^{m}{(\frac{1}{1+\exp(-\theta^T x^{(i)})})}^{y(i)}{(\frac{\exp(-\theta^T x^{(i)})}{1+\exp(-\theta^T x^{(i)})})}^{1-y(i)}}$


$z = {\theta}^{T}x^{(i)} \Rightarrow$


$L(\theta) = {\displaystyle \prod_{i=1}^{m}{(\frac{1}{1+\exp(-z)})}^{y(i)}{(\frac{\exp(-z)}{1+\exp(-z)})}^{1-y(i)}} = {\displaystyle \prod_{i=1}^{m}{(\frac{1}{1+\exp(-z)})}^{y(i)}(\frac{\exp(-z)}{1+\exp(-z)}){(\frac{1+\exp(-z)}{\exp(-z)})}^{y(i)}} = {\displaystyle \prod_{i=1}^{m}\exp(-z)\frac{1}{1+\exp(-z)}{\frac{1}{\exp(-z)}}^{y(i)}} = {\displaystyle \prod_{i=1}^{m}(\frac{1}{1+\exp(-\theta^Tx^{(i)})})(\exp(-\theta^Tx^{(i)}))^{1-y(i)}}$



---

*Log likelihood:*

$L(\theta) = \log(L(\theta)) = {\displaystyle \sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log (1-h_\theta(x^{(i)}))} = $
$={\displaystyle \sum_{i=1}^{m}y^{(i)}\log (\frac{1}{1+\exp(-\theta^Tx^{(i)})}) + (1-y^{(i)})\log (\exp(-\theta^{T}x^{(i)}))} = $

$= {\displaystyle \sum_{i=1}^{m}\theta^Tx^{(i)}(y^{(i)}-1) - \log (1+\exp(-\theta^{T}x^{(i)}))}$



---


*Derivatives:*

Likelihood: $L(\theta) = {\displaystyle \prod_{i=1}^{m} h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)})^{1-y^{(i)}}}$

Log-likelihood: $L(\theta) = {\displaystyle \sum_{i=1}^{m}y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)}))}$

$\Rightarrow$ To get the derivative with respect to $\theta$, we use the chain rule:


Derivative of log-likelihood for one data point (x,y):


$\frac{\partial L(\theta)}{\partial \theta_j} = \frac{\partial y \log h_\theta(x)}{\partial \theta_j} + \frac{\partial (1-y)\log (1-h_\theta(x))}{\partial \theta_j} =$ 

$ = [\frac{y}{h_\theta(x)} - \frac{1-y}{1-h_\theta(x)}]\frac{\partial h_\theta(x)}{\partial \theta_j} = [\frac{y}{h_\theta(x)} - \frac{1-y}{1-h_\theta(x)}]h_\theta(x)(1-h_\theta(x))x_j=$

$= [\frac{y - h_\theta(x)}{h_\theta(x)(1-h_\theta(x))}]h_\theta(x)(1-h_\theta(x))x_j = [y-h_\theta(x)]x_j$

So the derivative for all datapoints is:

$\frac{\partial L(\theta)}{\partial \theta_j} = {\displaystyle \sum_{i=1}^{m}(y^{(i)} - h_\theta(x^{(i)})){x_j}^{(i)}}$


---


*Gradient ascent optimization:*

The small step that we continually take, given the training dataset, can be calculated as:

$\theta_j := \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta_j}$

$\theta_j := \theta_j + \alpha {\displaystyle \sum_{i=1}^{m}(y^{(i)} - h_\theta(x^{(i)})){x_j}^{(i)}}$

where $\alpha$ is the magnitude of the taken step size. If we keep updating $\theta$ using the equation above, we will reach the convergence on the best value of $\theta$.



---


*For the second part of the quastion:*


Gradient descent aims at minimizing an objective function: $\theta _j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial \theta_j}$

Gradient ascent aims at maximizing an objective function: $\theta _j := \theta_j + \alpha\frac{\partial J(\theta)}{\partial \theta_j}$

Machine learning has a cost function and they can either be concave or convex. If it is convex we use Gradient Descent and if it is concave we use Gradient Ascent. Now there are two cost functions for logistic regression. When we use the convex one we use gradient descent and try to find the local minimum and when we use the concave one we use gradient ascent and try to find the local maximum.

################# Do not write below this line #################

#### Exercise 1.b **(7 Points)** Implementation of logistic regression with Gradient Ascent

Code up the equations above to learn the logistic regression parameters. The dataset used here is created using the make_classification function present in the SkLearn library. $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.
"""

X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=5)
X.shape, y.shape

sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);

"""Adding a column of 1's to $X$ to take into account the zero intercept"""

x = np.hstack([np.ones((X.shape[0], 1)), X])

[x[:5,:],x[-5:,:]] # Plot the first and last 5 lines of x, now containing features x0 (constant=1), x1 and x2

[y[:5],y[-5:]] # Plot the first and last 5 lines of y

"""Define the sigmoid function "sigmoid", the function to compute the gradient of the log likelihood  "grad_l" and the gradient ascent algorithm.

################# Do not write above this line #################
"""

def sigmoid(x):
    '''
    Function to compute the sigmoid of a given input x.
    
    Input:
    x: it's the input data matrix. The shape is (N, H)

    Output:
    g: The sigmoid of the input x
    '''
    
    #####################################################
    ##                 YOUR CODE HERE                  ##
    #####################################################
    g = 1/(1+np.exp(-x))

    return g

def log_likelihood(theta,features,target):
    '''
    Function to compute the log likehood of theta according to data x and label y
    
    Input:
    theta: it's the model parameter matrix.
    features: it's the input data matrix. The shape is (N, H)
    target: the label array
    
    Output:
    log_g: the log likehood of theta according to data x and label y
    '''
    
    #####################################################
    ##                 YOUR CODE HERE                  ##
    #####################################################

    h_theta = predictions(features, theta)

    log_l = (target * np.log(h_theta)) + ((1 - target) * np.log((1-h_theta)))
    log_l = sum(log_l)/len(target)

    return log_l


def predictions(features, theta):
    '''
    Function to compute the predictions for the input features
    
    Input:
    theta: it's the model parameter matrix.
    features: it's the input data matrix. The shape is (N, H)
    
    Output:
    preds: the predictions of the input features
    '''
    
    #####################################################
    ##                 YOUR CODE HERE                  ##
    #####################################################
    preds = sigmoid(np.dot(features, theta))

    return preds


def update_theta(theta, target, preds, features, lr):
    '''
    Function to compute the gradient of the log likelihood
    and then return the updated weights

    Input:
    theta: the model parameter matrix.
    target: the label array
    preds: the predictions of the input features
    features: it's the input data matrix. The shape is (N, H)
    lr: the learning rate
    
    Output:
    theta: the updated model parameter matrix.
    '''
    #####################################################
    ##                 YOUR CODE HERE                  ##
    #####################################################
    
    for i in range(len(theta)):
      theta[i]  = theta[i] + lr * (np.sum(np.dot((target - preds), features[:,i])))/len(target)

    return theta

def gradient_ascent(theta, features, target, lr, num_steps):
    '''
    Function to execute the gradient ascent algorithm

    Input:
    theta: the model parameter matrix.
    target: the label array
    num_steps: the number of iterations 
    features: the input data matrix. The shape is (N, H)
    lr: the learning rate
    
    Output:
    theta: the final model parameter matrix.
    log_likelihood_history: the values of the log likelihood during the process
    '''

    #####################################################
    ##                 YOUR CODE HERE                  ##
    #####################################################
    log_likelihood_history = np.zeros(num_steps)
    for i in range(num_steps):
      preds = predictions(features, theta)
      log_l = log_likelihood(theta,features, target )
      log_likelihood_history[i] = log_l
      theta = update_theta(theta, target, preds,features, lr)
    
    return theta, log_likelihood_history

"""################# Do not write below this line #################

Check your grad_l implementation:
grad_l applied to the theta_test (defined below) should provide a value for log_l_test close to the target_value (defined below); in other words the error_test should be 0, up to machine error precision.
"""

target_value = -1.630501731599431

output_test  = log_likelihood(np.array([-7,4,1]),x,y)
# output_test
error_test=np.abs(output_test-target_value)

print("{:f}".format(error_test))

"""Let's now apply the function gradient_ascent and print the final theta as well as theta_history """

# Initialize theta0
theta0 = np.zeros(x.shape[1])

# Run Gradient Ascent method
n_iter=1000
theta_final, log_l_history = gradient_ascent(theta0,x,y,lr=0.5,num_steps=n_iter)
# 
print(theta_final)

"""Let's plot the log likelihood over iterations

"""

fig,ax = plt.subplots(num=2)

ax.set_ylabel('l(Theta)')
ax.set_xlabel('Iterations')
_=ax.plot(range(len(log_l_history)),log_l_history,'b.')

"""Plot the data and the decision boundary:"""

# Generate vector to plot decision boundary
x1_vec = np.linspace(X[:,0].min(),X[:,1].max(),2)

# Plot raw data
sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, data=X)

# Plot decision boundary
plt.plot(x1_vec,(-x1_vec*theta_final[1]-theta_final[0])/theta_final[2], color="red")
plt.ylim(X[:,1].min()-1,X[:,1].max()+1)
print(theta_final)
# Save the theta_final value for later comparisons
theta_GA = theta_final.copy()

"""################# Do not write above this line #################

Discuss these two points:
1. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?
2. Let's deeply analyze how the learning rate $\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results?

######**Question 1:**
**Yes**. We'd use gradient ascent to maximize a likelihood function, and gradient descent to minimize a cost function. Both gradient descent and ascent are practically the same.
It is quite obvious that the gradient descent update is the same as the gradient ascent one, only we are formulating it as "taking a step into the opposite direction of the gradient of the cost function".

Also, if we add a minus before a convex function it becomes concave and vice versa. So in this way we can use gradient descent instead of gardient ascent.

######**Question 2:**

**$\alpha$:** Learning rate is used to scale the magnitude of parameter updates during gradient ascent. The choice of the value for learning rate can impact how fast the algorithm learn and whether the likelihood function is maximized or not.If our learning rate is set too low, training will progress very slowly as we are making very tiny updates to the weights in our network. However, if our learning rate is set too high, it can cause undesirable divergent behavior in our function.


Some of the results are summerized in the table below.


|  | $\alpha$ | Iteration | Theta_final |
|---|---|---|---|
| 1. |  0.005| 100 |[-0.00268696  0.22195954  0.00465272]  |
| 2. |  0.005| 200 | [-0.00502702  0.40490586  0.00714659] |
| 3. |  0.005| 500 | [-0.01306687  0.8033334   0.01010402]|
| 4. |  0.005| 1000 |[-0.03277276  1.21397715  0.01481238] |
| 5. | 0.01| 100 | [-0.00503012  0.40522709  0.00716239] |
| 6. | 0.01| 200 | [-0.01006669  0.68985982  0.00940272] |
| 7. | 0.01| 500 | [-0.0327839   1.21440684  0.01482183] |
| 8. | 0.01| 1000 |[-0.08234382  1.67614724  0.03157964] |
| 9. | 0.03| 100 | [-0.01645117  0.90596348  0.01088814] |
| 10. | 0.03| 200 | [-0.04219886  1.33464364  0.01757565] |
| 11. | 0.03| 500 | [-0.1318275   1.94910427  0.05218541] |
| 12. | 0.03| 1000 |[-0.24853549  2.37382206  0.1094709 ] |
| 13. | 0.05| 100 | [-0.03287392  1.21786078  0.01489813] |
| 14. | 0.05| 200 | [-0.08256927  1.67882336  0.03167618] |
| 15. | 0.05| 500 | [-0.21521639  2.27044902  0.09212719] |
| 16. | 0.05| 1000 |[-0.34162143  2.6230544   0.16101878] |
| 17. |  0.1 | 100 | [-0.08285485  1.68218857  0.03179853] |
| 18. |  0.1 | 200 |[-0.1768421   2.13706906  0.07305393]|
| 19. |  0.1 | 500 | [-0.34192964  2.62387055  0.16119107] |
| 20. |  0.1 | 1000 | [-0.43062684  2.83154211  0.21330677] |
| 21. |  0.5 | 100 | [-0.34444083  2.63047235  0.16259823] |
| 22. |  0.5 | 200 |[-0.43149445  2.83352547  0.21382358]|
| 23. |  0.5 | 500 | [-0.46038436  2.89903306  0.2311173 ] |
| 24. |  0.5 | 1000 | [-0.46097042  2.90036399  0.23146846] |




* **Decision boundary** is a property not of the training set, but the hypothesis and the parameters. Once we have the parameters theta, that is what defines the decision boundary. 

- As we had seen the result of the code by runnig it with different numbers of learning rate and iteration, the decsion boundary has changed because the parameter theta changed, when we change the number of iteration or change the value of $\alpha$ the slope size of the line that defines the decision boundary is changed. So the decsion boundry is influenced by these parameters change.

################# Do not write below this line #################

## Question 2: Logistic Regression with non linear boundaries (7 points)

#### Exercise 2.a **(4 Points)** Polynomial features for logistic regression

Define new features, e.g. of 2nd and 3rd degrees, and learn a logistic regression classifier by using the new features, by using the gradient ascent optimization algorithm you defined in Question 1.

In particular, we would consider a polynomial boundary with equation:

$f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2$

We would therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.

Create new arrays by stacking x and the new 7 features (in the order x1x1, x2x2, x1x2, x1x1x1, x2x2x2, x1x1x2, x1x2x2). In particular create x_new_quad by additionally stacking with x the quadratic features, and x_new_cubic by additionally stacking with x the quadratic and the cubic features.
"""

from sklearn.datasets import make_classification

X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=5)
X.shape, y.shape

x = np.hstack([np.ones((X.shape[0], 1)), X])

import seaborn as sns
import matplotlib.pyplot as plt
sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);

# First extract features x1 and x2 from x and reshape them to x1 vector arrays
x1 = x[:,1]
x2 = x[:,2]
x1 = x1.reshape(x1.shape[0], 1)
x2 = x2.reshape(x2.shape[0], 1)
print(x[:5,:]) # For visualization of the first 5 values
print(x1[:5,:]) # For visualization of the first 5 values
print(x2[:5,:]) # For visualization of the first 5 values

"""################# Do not write above this line #################

Your code here
"""

def new_features(x, degree=2):
    '''
    Function to create n-degree features from the input 

    Input:
    x: the initial features
    degree: the maximum degree you want the features
    
    Output:
    features: the final features. 
                                              [1, x1, x2, x1x1, x2x2, x1x2, x1x1x1, x2x2x2, x1x1x2, x1x2x2]
              2nd degree features must have the order [x, x1x1, x1x2, x2x2]
              3rd degree features must have the order [x, x1x1, x1x2, x2x2, x1x1x1, x1x1x2, x1x2x2, x2x2x2]
     '''
    #####################################################
    ##                 YOUR CODE HERE                  ##
    #####################################################
    x1 = x[:,1]
    x2 = x[:,2]
    
    features = np.ones(x1.shape[0])
    for i in range(1,degree + 1):
        for j in range(i + 1):
            features = np.column_stack((features, (x1 ** (i-j)) * (x2 ** j)))

            # i=1,j=0 => features = [x1]
            # i=1,j=1 => features = [x1, x2]
            # i=2,j=0 => features = [x1, x2, x1x1]
            # i=2,j=1 => features = [x1, x2, x1x1, x1x2]
            # i=2,j=2 => features = [x1, x2, x1x1, x1x2, x2x2]
            # i=3,j=0 => features = [x1, x2, x1x1, x1x2, x2x2, x1x1x1]
            # i=3,j=1 => features = [x1, x2, x1x1, x1x2, x2x2, x1x1x1, x1x1x2]
            # i=3,j=2 => features = [x1, x2, x1x1, x1x2, x2x2, x1x1x1, x1x1x2, x1x2x2]
            # i=3,j=3 => features = [x1, x2, x1x1, x1x2, x2x2, x1x1x1, x1x1x2, x1x2x2, x2x2x2]
     
    return features

"""################# Do not write below this line #################"""

x_new_quad = new_features(x, degree=2)
x_new_cubic = new_features(x, degree=3)

#reordering output features
temp = np.copy(x_new_quad[:, -1])
x_new_quad[:, -1] = x_new_quad[:, -2]
x_new_quad[:, -2] = temp

temp = np.copy(x_new_cubic[:, -1])
x_new_cubic[:, -1] = x_new_cubic[:, -2]
x_new_cubic[:, -2] = x_new_cubic[:, -3]
x_new_cubic[:, -3] = temp

"""Now use the gradient ascent optimization algorithm to learn theta by maximizing the log-likelihood, both for the case of x_new_quad and x_new_cubic."""

# Initialize theta0, in case of quadratic features
theta0_quad = np.zeros(x_new_quad.shape[1])

theta_final_quad, log_l_history_quad = gradient_ascent(theta0_quad,x_new_quad,y,lr=0.5,num_steps=n_iter)

# Initialize theta0, in case of quadratic and cubic features
theta0_cubic = np.zeros(x_new_cubic.shape[1])

# Run Newton's method, in case of quadratic and cubic features
theta_final_cubic, log_l_history_cubic = gradient_ascent(theta0_cubic,x_new_cubic,y,lr=0.5,num_steps=n_iter)

# check and compare with previous results
print(theta_final_quad)
print(theta_final_cubic)

# Plot the log likelihood values in the optimization iterations, in one of the two cases.
fig,ax = plt.subplots(num=2)

ax.set_ylabel('l(Theta)')
ax.set_xlabel('Iterations')
_=ax.plot(range(len(log_l_history_quad)),log_l_history_quad,'b.')

"""#### Exercise 2.b **(3 Points)** Plot the computed non-linear boundary and discuss the questions

First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2.

################# Do not write above this line #################

Your code here
"""

def boundary_function(x1_vec, x2_vec, theta_final):
    
    x1_vec, x2_vec = np.meshgrid(x1_vec,x2_vec)

    x1 = np.ravel(x1_vec)
    x2 = np.ravel(x2_vec)

    one_x1 = np.column_stack( (np.ones(x1.shape) , x1))
    one_x1_x2 = np.column_stack((one_x1, x2))

    if len(theta_final) == 6:
      c_0, c_1, c_2, c_3, c_4, c_5 = theta_final
      features = new_features(one_x1_x2, 2)
      f = c_0 * features[:,0] + c_1 * features[:,1] + c_2 * features[:,2] + c_3 * features[:,3] +c_5 * features[:,4] +c_4 * features[:,5]

      # gives us the results but the parameters are not in the correct order
      # f = np.dot(features, theta_final)

    elif len(theta_final) == 10:
      c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9 = theta_final
      features = new_features(one_x1_x2, 3)
      f = c_0 * features[:,0] + c_1 * features[:,1] + c_2 * features[:,2] + c_3 * features[:,3] +c_5 * features[:,4] +c_4 * features[:,5] + \
      c_6 * features[:, 6] + c_7 * features[:, 9] + c_8 * features[:,7] + c_9 * features[:, 8]
      
      # gives us the results but the parameters are not in the correct order
      # f = np.dot(features, theta_final)

    else:
        raise("Number of Parameters is not correct")

    f = f.reshape((len(x1_vec), len(x2_vec)))

    return x1_vec, x2_vec , f

"""################# Do not write below this line #################

Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions.
"""

x1_vec = np.linspace(X[:,0].min()-1,X[:,0].max()+1,200);
x2_vec = np.linspace(X[:,1].min()-1,X[:,1].max()+1,200);

x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta_final_quad)

sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, data=X);

plt.contour(x1_vec, x2_vec, f, colors="red", levels=[0])

plt.show()

x1_vec = np.linspace(X[:,0].min()-1,X[:,0].max()+1,200);
x2_vec = np.linspace(X[:,1].min()-1,X[:,1].max()+1,200);

x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta_final_cubic)

sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, data=X);

plt.contour(x1_vec, x2_vec, f, colors="red", levels=[0])
plt.show()

"""#### Confusion Matrix

Here you can see the confusion matrices related to the three models you've implemented. 

"""

from sklearn.metrics import confusion_matrix

## logistic regression with linear buondary

z = np.dot(x,theta_final)
probabilities = sigmoid(z)
y_hat = np.array(list(map(lambda x: 1 if x>0.5 else 0, probabilities)))
confusion_matrix(y, y_hat)

## logistic regression with non linear buondary - quadratic

z = np.dot(x_new_quad,theta_final_quad)
probabilities = sigmoid(z)
y_hat = np.array(list(map(lambda x: 1 if x>0.5 else 0, probabilities)))
confusion_matrix(y, y_hat)

## logistic regression with non linear buondary - cubic

z = np.dot(x_new_cubic,theta_final_cubic)
probabilities = sigmoid(z)
y_hat = np.array(list(map(lambda x: 1 if x>0.5 else 0, probabilities)))
confusion_matrix(y, y_hat)

"""################# Do not write above this line #################

Write now your considerations. Discuss in particular:
- Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?
- Let's now delve into some quantitative analysis. The three tables you have generated represent the confusion matrix for the model you have implemented in the first two questions. What can you say about actual performances? Does the increase of the degree have a high effect on the results?

**1.**  As we can see in the above figures, by increasing the order of the polynomial we could represent more complex form of decision boundary. Since the dataset is not linearly separable, linear regression might struggle to deliver a good performance and we could end up in underfitting. When we have nonlinear relations, we often assume a linear model and then we fit data to the model using polynomial regression. Clearly, it can be seen the model's performance improves in splitting two classes as we increase 
the order. **However**, this improvement might not be significant and increasing the order carelessly in order to obtain a better performance can lead us to over-fitting. Increasing the order of the polynomial, increases the potential of over-fitting the losing generalization ability of the model. In fact we could get worse results with respect to the original linear model with too high-order models.



**2.** As we increase the degree of the polynomial, the model can represent more complexity. According to the confusion matrix, it improves our model, but when we dug into the confusion matrix, we figured out that the improvement of our model is not significant (e.g. 88% linear model, 90% quadratic model and 92% cubic model regarding accuracy metric). In fact, for bigger data sets, time and memory consumption can be a problem in case of high order polynomials. In a nutshell, here we had improvments (eventhough not noticeable), we could try higher orders since the size of the dataset was small so the actuall performace improved slightly. But, if we had a bigger dataset, we could say that the high amount of calculations are not worth that small improvement.

################# Do not write below this line #################

## Question 3: Multinomial Classification (Softmax Regression) **(13 Points)**

### Code and Theory **(10 Points)**
### Report **(3 Points)**

#### Exercise 3.a **(4 Points)**

In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\in\{1,...,K\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K. 
In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:

\begin{equation}
s_i =  X_i \theta
\end{equation}

Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:

\begin{equation}
p_{ik} = \frac{\exp(X_i \theta_k)}{\sum_{j=1}^K(X_i \theta_j))}
\end{equation}

We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*. 
In the first of this exercise we have to: 
-    Write the equations of the Cross Entropy loss for the Softmax regression model;
-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.

#### A bit of notation

*  N: is the number of samples 
*  K: is the number of classes
*  X: is the input dataset and it has shape (N, H) where H is the number of features
*  y: is the output array with the labels; it has shape (N, 1)
*  $\theta$: is the parameter matrix of the model; it has shape (H, K)

################# Do not write above this line #################

Your equations here.

\begin{equation}
L(\theta) = -\frac{1}{N}\displaystyle \sum_{i=1}^{N} y_i . \log(p_i)
\end{equation}


\begin{equation}
\nabla_{\theta_k} L(\theta) = \frac{1}{N}\displaystyle \sum_{i=1}^{N}(p_i-y_i).x_i
\end{equation}

################# Do not write below this line #################

#### Exercise 3.b **(4 Points)**

Now we will implement the code for the equations. Let's implement the functions:
-  softmax 
-  CELoss
-  CELoss gradient
-  gradient descent

We generate a toy dataset with *sklearn* library. Do not change anything outside the parts provided of your own code (else the provided checkpoint will not work).
"""

from sklearn.datasets import make_classification

X, y = make_classification(n_samples=300, n_features=7, n_informative=7, n_redundant=0, n_classes=3, random_state=1)
X.shape, y.shape

"""As a hint for the implementations of your functions: consider the labels $y$ as one-hot vector. This will allow matrix operations (element-wise multiplication and summation)."""

import scipy
import numpy as np

def class2OneHot(vec):
    out_sparse = scipy.sparse.csr_matrix((np.ones(vec.shape[0]), (vec, np.array(range(vec.shape[0])))))
    out_onehot = np.array(out_sparse.todense()).T
    return out_onehot

y_onehot = class2OneHot(y)

"""Let's visualize the generated dataset. We use as visualizzation method the *Principal Component Analysis* (PCA). PCA summarize the high-dimensional feature vectors of each sample into 2  features, which we can illustrate with a 2D plot. Look at the following plot, the 3 generated classes do not seem separable."""

from sklearn.decomposition import PCA
import pandas as pd

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X)
principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])
finalDf = pd.concat([principalDf, pd.DataFrame(y, columns = ['target'])], axis = 1)

import seaborn as sns
import matplotlib.pyplot as plt
sns.scatterplot(x='pc1', y='pc2', hue='target', data=finalDf);

"""################# Do not write above this line #################"""

def softmax(theta, X):
    '''
    Function to compute associated probability for each sample and each class.
    
    Input:
    theta: it's the model parameter matrix. The shape is (H, K)
    X: it's the input data matrix. The shape is (N, H)

    Output:
    softmax: it's the matrix containing probability for each sample and each class. The shape is (N, K)
    '''

    softmax= np.exp(np.dot(X,theta))
    Denominator = np.sum(softmax,axis=1)
    for i in range(X.shape[0]):
        softmax[i,:]= softmax[i,:]/Denominator[i] 
        
    return softmax
    


def CELoss(theta, X, y_onehot):
    '''
    Function to compute softmax regression model and Cross Entropy loss.
    
    Input:
    theta: it's the model parameter matrix. The shape is (H, K)
    X: it's the input data matrix. The shape is (N, H)
    y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)

    Output:
    loss: The scalar that is the mean error for each sample.
    '''
    loss = 0
    s = softmax(theta,X)
    for i in range(X.shape[0]):
        loss += np.dot(y_onehot[i], np.log(s[i]))

        
    return -loss/X.shape[0]


def CELoss_jacobian(theta, X, y_onehot):
    '''
    Function to compute gradient of the cross entropy loss with respect the parameters.
    
    Input:
    theta: it's the model parameter matrix. The shape is (H, K)
    X: it's the input data matrix. The shape is (N, H)
    y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)

    Output:
    jacobian: A matrix with the partial derivatives of the loss. The shape is (H, K)
    '''

    s = softmax(theta, X)
    jacobian = np.zeros(theta.shape)

    for i in range(theta.shape[0]):
        for j in range(theta.shape[1]):        
            jacobian[i,j] = np.sum(np.multiply(s[:,j] - y_onehot[:,j], X[:, i])) 
    return jacobian/ X.shape[0]


def gradient_descent(theta, X, y_onehot, alpha=0.01, iterations=100):
    '''
    Function to compute gradient of the cross entropy loss with respect the parameters.
    
    Input:
    theta: it's the model parameter matrix. The shape is (H, K)
    X: it's the input data matrix. The shape is (N, H)
    y_onehot: it's the label array in encoded as one hot vector. The shape is (N, K)
    alpha: it's the learning rate, so it determines the speed of each step of the GD algorithm
    iterations: it's the total number of step the algorithm performs

    Output:
    theta: it's the updated matrix of the parameters after all the iterations of the optimization algorithm. The shape is (H, K)
    loss_history: it's an array with the computed loss after each iteration
    '''

    loss_history = np.zeros(iterations)
    for i in range(iterations):
        theta = theta - alpha * CELoss_jacobian(theta, X, y_onehot)
        loss_history[i] = CELoss(theta, X, y_onehot)
    
        
    return theta, loss_history

"""################# Do not write below this line #################"""

# Initialize a theta matrix with random parameters
theta0 = np.random.rand(X.shape[1], len(np.unique(y)))

print("Initial Loss with initialized theta is:", CELoss(theta0, X, y_onehot))

# Run Gradient Descent method
n_iter = 1000
theta_final, log_l_history = gradient_descent(theta0, X, y_onehot, alpha=0.01, iterations=n_iter)

theta_final

loss = CELoss(theta_final, X, y_onehot)
loss

# alpha= 0.01
fig,ax = plt.subplots(num=2)

ax.set_ylabel('loss')
ax.set_xlabel('Iterations')
_=ax.plot(range(len(log_l_history)), log_l_history,'b.')

"""#### Exercise 3.c **(2 Points)**

Let's now evaluate the goodness of the learnt based on accuracy:

\begin{equation}
Accuracy = \frac{Number\ of\ correct\ predictions}{Total\ number\ of\ predictions}
\end{equation}

Implement the compute_accuracy function. You may compare the accuracy achieved with learnt model Vs. a random model (random $\Theta$) or one based on $\Theta$'s filled with zeros.

################# Do not write above this line #################
"""

def compute_accuracy(theta, X, y):
    '''
    Function to compute accuracy metrics of the softmax regression model.
    
    Input:
    theta: it's the final parameter matrix. The one we learned after all the iterations of the GD algorithm. The shape is (H, K)
    X: it's the input data matrix. The shape is (N, H)
    y: it's the label array. The shape is (N, 1)

    Output:
    accuracy: Score of the accuracy.
    '''

    TruePositive = 0 
    s  = softmax(theta, X)
    for i in range(s.shape[0]):
      Vector = list(s[i, :])
      MaxIndex = Vector.index(max(Vector))
      
      TruePositive += (0 if y[i] != MaxIndex else 1)
    
    accuracy = TruePositive/X.shape[0]

    return accuracy

"""################# Do not write below this line #################"""

compute_accuracy(theta_final, X, y)

theta0 = np.random.rand(X.shape[1], len(np.unique(y)))
compute_accuracy(theta0, X, y)

compute_accuracy(np.zeros((X.shape[1], len(np.unique(y)))), X, y)

"""### Report **(3 Points)**

Experiment with different values for the learning rate $\alpha$ and the number of iterations. Look how the loss plot changes the convergence rate and the resulting accuracy metric. Report also execution time of each run. For this last step you could you %%time at the beginning of the cell to display time needed for the algorithm.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Initialize a theta matrix with random parameters
# theta0 = np.random.rand(X.shape[1], len(np.unique(y)))
# 
# print("Initial Loss with initialized theta is:", CELoss(theta0, X, y_onehot))
# 
# # Run Gradient Descent method
# n_iter = 100
# theta_final, log_l_history = gradient_descent(theta0, X, y_onehot, alpha=0.001, iterations=n_iter)

"""**Write your Report here**


| LR | Iter | Accuracy | Time |
|---|---|---|---|
0.005| 100 | 0.67 |275ms|
0.005| 200 |0.756 |548ms
0.005| 500 |0.75 |1.35s
0.005| 1000 |0.773 |2.73s
0.01| 100 | 0.7 |277ms
0.01| 200 | 0.713 |561ms
0.01| 500 | 0.77 |1.35s
0.01| 1000 |0.793 |2.38s
0.03| 100 | 0.753 |298ms
0.03| 200 |0.77 |592ms
0.03| 500 |0.796 |1.42s
0.03| 1000 |0.79 |2.96s
0.05| 100 | 0.77 |301ms
0.05| 200 |0.79 |608ms
0.05| 500 |0.793  |1.45s
0.05| 1000 | 0.786|2.88s
0.1 | 100 |0.786  |301ms
0.1 | 200 |0.796|611ms
0.1 | 500 | 0.786 |1.41s
0.1 | 1000 | 0.786|2.91s
0.5 | 100 | 0.786 |300ms
0.5 | 200 |0.786|577ms
0.5 | 500 |0.786 |1.46s
0.5 | 1000 |0.786 |2.9s



"""

# Run Gradient Descent method
loss_history_custom = [0] * 6
alpha = [0.005, 0.01, 0.03, 0.05, 0.1, 0.5]
for (i,al) in enumerate(alpha):
  _, loss_history_custom[i] = gradient_descent(theta0, X, y_onehot, alpha=al, iterations=200)

plt.rcParams["figure.figsize"] = (15 , 10)
fig, ax = plt.subplots(2, 3)


# Plot the elbow method's values
for i in range(2):
  for j in range(3):
    ax[i,j].plot(range(200) , loss_history_custom[i*3+j], 'b.' ,linewidth = 3)
    ax[i,j].set_title("alpha=" + str(alpha[i*3+j]), fontsize = 14)
plt.show()

"""As we can see in the figures above, by choosing the smaller learning rate (alpha) the algorithm will reach the minimum loss slower. So in order to reach the minimum loss with smaller alpha we need more number of iterations.
In theory, we need to be careful about increasing the learning rate as maybe the algorithm can diverge, but we have not seen this behavior in our analysis even by choosing the high values as learning rate.

## Question 4: Multinomial Naive Bayes **(6 Points)**

### Code and Theory

The Naive Bayes classifier is a probabilistic machine learning model often used for classification tasks, e.g. document classification problems.

In the multinomial Naive Bayes classification you generally have $K>2$ classes, and the features are assumed to be generated from a multinomial distribution.

##### __*Example Data*__
General models consider input data as values. In the case of MultinomialNB, being used mainly in the field of document classification, these data consider how many features $X_i$ are present in the sample. Basically, it is a count of features within each document.

Taking into account $D=3$ documents and a vocabulary consisting of $N=4$ words, the data are considered as follows.

|  | $w_1$ | $w_2$ | $w_3$ | $w_4$ |
|---|---|---|---|---|
| $d_1$  | 3 | 0 | 1 | 1 |
| $d_2$ | 2 | 1 | 3 |0|
| $d_3$ | 2 | 2 | 0 |2|

By randomly generating the class to which each document belongs we have $y=[1,0,1]$



##### __*A bit of notation*__
- $Y =\{y_1, y_2, ... , y_{|Y|}\}$: set of classes

- $V =\{w_1, w_2, ... , w_{|V|}\}$: set of vocabulary

-  $D =\{d_1, d_2, ... , d_{|D|}\}$: set of documents 

-  $N_{yi}$:  count of a specific word $w_i$ in each unique class, e.g. for $y=1$ you select $D_1$ and $D_3$, then for third column  you have $N_{y,3}=1$ 
-  $N_y$: total count of features for a specific class, e.g. for $y=1$ you sum all rows values which the correspondent label is 1, so $N_y=11$ 

-  $n$: total number of features (words in vocabulary)
-  $\alpha$: smoothing parameters

##### __*Task*__
Find the class $y$ to which the document is most likely to belong given the words $w$.
Use the Bayes formula and the posterior probability for it.

Bayes Formula:
\begin{equation}
P(A|B) = \frac{P(A)*P(B|A)}{P(B)}
\end{equation}

Where:
- P(A): Prior probability of A
- P(B): Prior probability of B
- P(B|A): Likelihood, multiplying posterior probability, that is multinomial Naive Bayes is:
\begin{equation}
P(B|A) = \left(\frac{N_{yi}+\alpha}{N_{y}+\alpha*n\_features}\right)^{X_{doc,i}}
\end{equation}

**Reminder: do not change any part of this notebook outside the assigned work spaces**

#### Generate random dataset
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, y = make_classification(n_samples=300, n_features=7, n_informative=7, n_redundant=0, n_classes=3, random_state=1)
X = np.floor(X)-np.min(np.floor(X))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, y_train.shape

"""#### Step0: $N_y$ and $N_{yi}$"""

def feature_count(X, y, classes, n_classes, n_features):
        '''
        Function to compute the count of a specific word in each unique class and the total count.
        
        Input:
        X: it's the input data matrix.
        y: label array
        classes: unique values of y
        n_classes: number of classes
        n_features: it's the number of word in Vocabulary.

        Output:
        N_yi:   count of a specific word $w_i$ in each unique class
        N_y: total count of features for a specific class
        '''
        N_yi = np.zeros((n_classes, n_features)) # feature count
        N_y = np.zeros((n_classes)) # total count 

        #####################################################
        ##                 YOUR CODE HERE                  ##
        #####################################################
        for i in range(n_classes):

            for j in range(n_features):
              X_temp = X[np.where(y==i)]
              N_yi[i,j] = np.sum(X_temp[:,j])
              
            N_y[i] = np.sum(N_yi[i,:])
        return N_yi, N_y

n_samples_train, n_features = X_train.shape
classes = np.unique(y_train)
n_classes = 3
alpha = 0.1

N_yi, N_y = feature_count(X_train, y_train, classes, n_classes, n_features)

"""#### Step1: Prior Probability
The probability of a document being in a specific category from the given set of documents.

######################################

Your equations here:
\begin{equation}
P(y_j) = \frac{\text{Number of desired outcomes}}{\text{Total number of outcomes}}
\end{equation}

######################################

"""

def prior_(X, y, n_classes, n_samples):
        """
        Calculates prior for each unique class in y.

        Input:
        X: it's the input data matrix.
        y: label array
        n_classes: number of classes
        n_samples: number of documents

        Output:
        P: prior probability for each class. Shape: (, n_classes)
        """
        classes = np.unique(y)
        P = np.zeros(n_classes)

        # Implement Prior Probability P(A)
        #####################################################
        ##                 YOUR CODE HERE                  ##
        #####################################################
        _, count = np.unique(y, return_counts=True)
        for i in range(len(classes)):
          P[i]= count[i]/ n_samples

        return P

prior_prob = prior_(X_train, y_train, n_classes, n_samples_train)
print(prior_prob)

"""#### Step2
Posterior Probability: The conditional probability of a word occurring in a document given that the document belongs to a particular category.

\begin{equation}
P(w_i|y_j) = \left(\frac{N_{yi}+\alpha}{N_{y}+\alpha*n\_features}\right)^{X_{doc,i}}
\end{equation}


Likelihood for a single document: 
######################################

Your equations here:
\begin{equation}
P(w|y_j) = \prod_{i=1}^{n}P(w_i|y_j)= \prod_{i=1}^{n}\left(\frac{N_{yi}+\alpha}{N_{y}+\alpha*n\_features}\right)^{X_{doc,i}}
\end{equation}

######################################

"""

def posterior_(x_i, i, h, N_y, N_yi, n_features, alpha):
        """
        Calculates posterior probability. aka P(w_i|y_j) using equation in the notebook.
        
        Input:
        x_i: feature x_i
        i: feature index.  
        h: a class in y
        N_yi:   count of a specific word in each unique class
        N_y: total count of features for a specific class
        n_features: it's the number of word in Vocabulary.
        alpha: smoothing parameter

        Output:
        posterior: P(xi | y). Float.
        """

        # Implement Posterior Probability
        #####################################################
        ##                 YOUR CODE HERE                  ##
        #####################################################
        posterior = ((N_yi[h,i] + alpha)/(N_y[h] + (alpha) * n_features))**x_i
        return posterior
    
def likelihood_(x, h, N_y, N_yi, n_features, alpha):
        """
        Calculates Likelihood P(w|j_i).
        
        Input:
        x: a row of test data. Shape(n_features,)
        h: a class in y
        N_yi:   count of a specific word in each unique class
        N_y: total count of features for a specific class
        n_features: it's the number of word in Vocabulary.
        alpha: smoothing parameter

        Output:
        likelihood: Float.
        """
        
        tmp = []
        for i in range(x.shape[0]):
            tmp.append(posterior_(x[i], i, h, N_y, N_yi, n_features, alpha))
        likelihood = np.prod(tmp)
        # Implement Likelihood
        #####################################################
        ##                 YOUR CODE HERE                  ##
        #####################################################
        
        return likelihood

# Example of likelihood for first document
likelihood_(X_test[0], 0, N_y, N_yi, n_features, alpha)

"""#### Step3
Joint Likelihood that, given the words, the documents belongs to specific class
######################################

Your equations here:
\begin{equation}
P(y_i|w) = P(w|y_j)P(y)
\end{equation}

######################################

Finally, from the probability that the document is in that class given the words, take the argument correspond to max value.

\begin{equation}
y(D) = argmax_{y \in Y} \frac{P(y|w)}{\sum_{j}P(y_j|w)}
\end{equation}
"""

def joint_likelihood(X, prior_prob, classes, n_classes, N_y, N_yi, n_features, alpha):
        """
        Calculates the joint probability P(y_i|w) for each class and makes it probability.
        Then take the argmax.
        
        Input:
        X: test data
        prior_prob:
        classes:
        n_classes:
        N_yi:   count of a specific word in each unique class
        N_y: total count of features for a specific class
        n_features: it's the number of word in Vocabulary.
        alpha: smoothing parameter

        Output:
        predicted_class: Predicted class of the documents. Int. Shape: (,#documents)
        """

        
        # Calculate Joint Likelihood of each row for each class, then normalize in order to make them probabilities
        # Finally take the argmax to have the predicted class for each document
        #####################################################
        ##                 YOUR CODE HERE                  ##
        #####################################################

        samples, features = X.shape
        predict_proba = np.zeros((samples,n_classes))
        predicted_class = np.zeros(samples) 

        for i in range(samples):
          for h in classes:
            predict_proba[i][h] = prior_prob[h] * likelihood_(X[i], h, N_y, N_yi, n_features, alpha)

          predict_proba[i] /= np.sum(predict_proba[i])
          predicted_class[i] = np.where(predict_proba[i] == predict_proba[i].max())[0]
        
        return predicted_class

yhat = joint_likelihood(X_test, prior_prob, classes, n_classes, N_y, N_yi, n_features, alpha)

"""#### Step4: Calculate the Accuracy Score"""

print('Accuracy: ', np.round(accuracy_score(yhat, y_test),3))

"""**Sanity Check**

Here we use a function from the sklearn library, one of the most widely used in machine learning. MultinomialNB() implements the required algorithm, so the result of your implementation should be equal to the output of the following function.
"""

from sklearn import naive_bayes
clf = naive_bayes.MultinomialNB(alpha=0.1)
clf.fit(X_train,y_train)
sk_y = clf.predict(X_test)
print('Accuracy: ', np.round(accuracy_score(sk_y, y_test),3))